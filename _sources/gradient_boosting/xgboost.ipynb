{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team members:\n",
    "* Project Manager - **Adilzhan Jumakanov**\n",
    "* Technical writer - **Abylay Aitbanov**\n",
    "* Author of executable content - **Alisher Aip**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Book about XGBoost\n",
    "<a href=\"https://drive.google.com/file/d/1Lf8lbV5OiFa-EiJWToSIyjHOx9oBe0Hg/view?usp=sharing\"> Read here </a>\n",
    "<br>\n",
    "<div>\n",
    "  <img style=\"width: 100%\" src=\"images/img_1.png\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[XGBoost](https://github.com/dmlc/xgboost) is one of the most popular and efficient implementations of the Gradient Boosted Trees algorithm, a supervised learning method that is based on function approximation by optimizing specific loss functions as well as applying several regularization techniques. It is an ensemble learning method that combines the predictions of multiple weak models to produce a stronger prediction. \n",
    "\n",
    "XGBoost stands for “Extreme Gradient Boosting” and it has become one of the most popular and widely used machine learning algorithms due to its ability to handle large datasets and its ability to achieve state-of-the-art performance in many machine learning tasks such as classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Objective Function in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The objective function in XGBoost is designed to be minimized during the training process. For a regression problem, it is commonly defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{Objective} = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "Here's a breakdown of the components:\n",
    "\n",
    "\n",
    "1. $$ \\(\\sum_{i=1}^{n} L(y_i, \\hat{y}_i)\\) $$\n",
    "   - This term represents the sum of the loss function over all data points.\n",
    "   - \\(n\\) is the number of data points.\n",
    "   - $ \\(L(y_i, \\hat{y}_i)\\) $ is the loss incurred for predicting $ \\(\\hat{y}_i\\) $ when the true label is $ \\(y_i\\) $.\n",
    "\n",
    "  \n",
    "2. $$ \\(\\sum_{k=1}^{K} \\Omega(f_k)\\) $$\n",
    "   - This term represents the sum of the regularization terms over all the trees in the ensemble.\n",
    "   - $ \\(K\\) $ is the number of trees.\n",
    "   - $ \\(\\Omega(f_k)\\) $ is the regularization term for the $ \\(k\\) $-th tree.\n",
    "\n",
    "   \n",
    "For a regression problem, a common choice for the loss function is the mean squared error (MSE), which is given by:\n",
    "\n",
    "$$\n",
    "L(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "The regularization term \\(\\Omega(f_k)\\) typically consists of two parts:\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Omega(f_k) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^{T} w_{j}^2\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ \\(T\\) $ is the number of leaves in the tree.\n",
    "- $ \\(w_j\\) $ is the score assigned to the \\(j\\)-th leaf.\n",
    "- $ \\(\\gamma\\) $ and $ \\(\\lambda\\) $ are regularization parameters.\n",
    "\n",
    "The entire objective function is designed to balance the model's fit to the training data (captured by the loss function) with a penalty for complexity (captured by the regularization term). It ensures that the model generalizes well to unseen data while avoiding overfitting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Description of the algorithm\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "  <img style=\"width: 100%\" src=\"images/img_2.png\" />\n",
    "</div>\n",
    "\n",
    "XGBoost is based on the gradient boosting algorithm for decision trees. Gradient boosting is a machine learning technique for classification and regression problems that builds a prediction model in the form of an ensemble of weak predictive models, usually decision trees. Ensemble training is carried out sequentially, in contrast to, for example, bagging. At each iteration, the deviations of the predictions of the already trained ensemble on the training set are calculated. The next model that will be added to the ensemble will predict these deviations. Thus, by adding the predictions of the new tree to the predictions of the trained ensemble, we can reduce the average deviation of the model, which is the target of the optimization problem. New trees are added to the ensemble until the error decreases or until one of the \"early stopping\" rules is satisfied.\n",
    "\n",
    "Consider an illustration of boosting. It examines the behavior of the model at one point of an abstract linear regression problem. Let us assume that the first model of the ensemble F always produces the sample mean of the predicted value f0. This prediction is quite rough, so the standard deviation at our chosen point will be quite large. We will try to fix this by training a model Δ1, which will “correct” the prediction of the previous ensemble F0. Thus, we obtain ensemble F1, the prediction of which will be summed up from the predictions of the f0 and Δ1 models. Continuing this sequence, we arrive at the ensemble F4, the prediction of which is summed up from the predictions f0, Δ1, Δ2, Δ3, Δ4 and predicts exactly the value of the given target."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBOOST Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Algorithm Enhancements:\n",
    "\n",
    "1. Tree [Pruning](https://www.displayr.com/machine-learning-pruning-decision-trees/) is a method employed in machine learning to decrease the size of regression trees. This is achieved by replacing nodes that do not contribute to enhancing classification on the leaves. The primary objective of pruning a regression tree is to prevent overfitting of the training data. The most effective approach for pruning is the Cost Complexity or Weakest Link Pruning, which utilizes mean square error, k-fold cross-validation, and learning rate internally. In the case of XGBoost, the algorithm generates nodes (or splits) up to a specified max_depth and initiates pruning from the end, progressing backward until the loss falls below a defined threshold. To decide whether to remove a split, XGBoost considers the cumulative loss by computing the total loss (-3 + 7 = +4) and retains both the split and subsequent node if the result is positive.\n",
    "\n",
    "\n",
    "2. Sparsity-Aware Split Discovery is a technique frequently employed when dealing with data that exhibits sparsity, characterized by a significant presence of missing or empty values. This sparsity may arise either inherently in the collected data or result from data engineering processes such as feature encoding. To account for sparsity patterns in the data, each tree is assigned a default direction. XGBoost addresses missing data by assigning them to the default direction and determining the optimal imputation value that minimizes the training loss. The optimization strategy involves selectively visiting only the missing values, resulting in a significant acceleration of the algorithm—up to 50 times faster than the naive method.\n",
    "\n",
    "\n",
    "### System Enhancements:\n",
    "\n",
    "1. Parallelization is a crucial aspect of tree learning, where data needs to be organized in a sorted manner. To mitigate the expenses associated with sorting, the data is partitioned into compressed blocks, each containing a column with its corresponding feature values. XGBoost enhances efficiency by concurrently sorting each block using all the available cores/threads of the CPU. This optimization is particularly beneficial because a substantial number of nodes are regularly generated in the process of building a tree. In essence, XGBoost achieves parallelization by distributing the workload of sequentially generating trees across multiple cores or threads.\n",
    "2. Through cache-aware optimization, gradient statistics (both direction and value) for each split node are stored in an internal buffer specific to each thread. The accumulation of these statistics is done in a mini-batch manner, effectively minimizing the time overhead associated with immediate read/write operations and preventing cache misses. The optimization strategy is rooted in achieving cache awareness, and this is facilitated by selecting an optimal block size, typically around 2¹⁶. In summary, the cache-aware approach enhances efficiency by strategically managing and utilizing cache resources, resulting in improved performance for the algorithm.\n",
    "\n",
    "\n",
    "### Flexibility in XGBoost:\n",
    "\n",
    "1. A customized objective function in machine learning serves the purpose of either maximizing or minimizing a specific criterion. In the context of machine learning, the goal is typically to minimize the objective function, which is a composite of the loss function and a regularization term. The objective function encapsulates the measure of how well the model performs, incorporating both the accuracy in predicting outcomes (loss function) and any regularization constraints imposed to prevent overfitting or enhance generalization. Customization of the objective function allows practitioners to tailor the optimization process according to the specific goals and characteristics of their machine learning task.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "  <img style=\"width: 100%\" src=\"images/img_4.png\" />\n",
    "</div>\n",
    "\n",
    " - Optimizing the loss function encourages predictive models whereas optimizing regularization leads to smaller variance and makes prediction stable. Different objective functions available in XGBoost are:\n",
    "\n",
    "    * reg: linear for regression\n",
    "    * reg: logistic, and binary: logistic for binary classification\n",
    "    * multi: softmax, and multi: softprob for multiclass classification\n",
    "\n",
    "2. Customized Evaluation Metric — This is a metric used to monitor the model’s accuracy on validation data.\n",
    "    ● rmse — Root mean squared error (Regression)\n",
    "    ● mae — Mean absolute error (Regression)\n",
    "    ● error — Binary classification error (Classification)\n",
    "    ● logloss — Negative log-likelihood (Classification)\n",
    "    ● auc — Area under the curve (Classification)\n",
    "   \n",
    "### Cross-validation\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "  <img style=\"width: 100%\" src=\"images/img_3.png\" />\n",
    "</div>\n",
    "\n",
    "1. Built-in Cross-validation - s a statistical approach for evaluating models on unfamiliar data, especially beneficial when dealing with limited datasets. It avoids the conventional practice of setting aside an independent sample for validation to prevent overfitting. The challenge arises when reducing the training data size for validation compromises the model's ability to grasp intricate features and patterns within the data, potentially leading to errors. This is akin to the functionality offered by scikit-learn's cross_val_score, which facilitates a robust assessment of a model's generalization performance by systematically dividing the data into subsets for training and evaluation, ensuring a more reliable estimate of performance on unseen data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE as a function of ridge regression (L2 regularization):\n",
      "    l2      rmse\n",
      "0    1  4.019746\n",
      "1   10  4.845198\n",
      "2  100  6.260982\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fetching the Boston housing prices dataset from the original source\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "# Creating a DMatrix from the fetched data\n",
    "boston_dmatrix = xgb.DMatrix(data=data, label=target)\n",
    "\n",
    "# Regularization parameters\n",
    "reg_params = [1, 10, 100]\n",
    "\n",
    "# XGBoost parameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"max_depth\": 3}\n",
    "\n",
    "# List to store RMSE values\n",
    "ridge_regression = []\n",
    "\n",
    "# Iterate over regularization parameters\n",
    "for reg in reg_params:\n",
    "    # Update lambda (L2 regularization) in parameters\n",
    "    params[\"lambda\"] = reg\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_results_rmse = xgb.cv(dtrain=boston_dmatrix, params=params, nfold=5, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "    # Append best RMSE (final round)\n",
    "    ridge_regression.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Display the best RMSE for each regularization parameter\n",
    "print(\"Best RMSE as a function of ridge regression (L2 regularization):\")\n",
    "print(pd.DataFrame(list(zip(reg_params, ridge_regression)), columns=[\"l2\", \"rmse\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T01:54:12.141125Z",
     "start_time": "2023-12-08T01:54:10.850118Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DMatrix is an internal data structure used by XGBoost which is optimized for memory efficiency and training speed. We need to transform our numpy array of data using DMatrix so that it can be later utilized in dtrain and dtest parameters of its inbuilt functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation cross-validation strategies like GridSearchCV and RandomizedSearchCV."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Grid Search — We pass on a parameter’s dictionary to the function and compare the cross-validation score for each combination of parameters (many to many) in the dictionary and return the set having the best parameters.\n",
    "\n",
    "2. Random Search — We draw a random value during each iteration from the range of specified values for each hyperparameter searched over, and evaluate a model with those hyperparameters. After completing all iterations, it picks the hyperparameter configuration with the best score.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 72 candidates, totalling 288 fits\n",
      "GridSearchCV\n",
      "Best parameters found:  {'colsample_bytree': 0.7, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Lowest RMSE found:  4.275231543259102\n",
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "Randomize Search Cross Validation\n",
      "Best parameters found:  {'subsample': 0.32, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.41000000000000003}\n",
      "Lowest RMSE found:  4.842244758327791\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# Fetching the Boston housing prices dataset from the original source\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Grid Search Parameters\n",
    "grid_search_params = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.5],\n",
    "    'n_estimators': [100],\n",
    "    'subsample': [0.2, 0.5, 0.8],\n",
    "    'max_depth': [2, 3, 5]\n",
    "}\n",
    "\n",
    "xg_grid_reg = xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
    "\n",
    "grid = GridSearchCV(estimator=xg_grid_reg, param_grid=grid_search_params, scoring='neg_mean_squared_error',\n",
    "                    cv=4, verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "print(\"GridSearchCV\")\n",
    "print(\"Best parameters found: \", grid.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid.best_score_)))\n",
    "\n",
    "# Random Search Parameters\n",
    "params_random_search = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.01),\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': range(2, 12),\n",
    "    'subsample': np.arange(0.02, 1.02, 0.02)\n",
    "}\n",
    "\n",
    "xg_random_reg = xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
    "\n",
    "randomized_mse = RandomizedSearchCV(param_distributions=params_random_search, estimator=xg_random_reg,\n",
    "                                    scoring=\"neg_mean_squared_error\", n_iter=5, cv=4, verbose=1)\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "print(\"Randomize Search Cross Validation\")\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T01:50:16.464468Z",
     "start_time": "2023-12-08T01:48:43.821653Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Total configurations in Grid Search → 2*4*1*3*3 = 72\n",
    "Total configurations in Random Search → 100*1*10*50 = 50000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extendibility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.  **Regression using XGBoost:**\n",
    "\n",
    "#### 2.1. Decision Tree Base Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8094614508392113\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "import os\n",
    "\n",
    "# Replace '/usr/local/bin' with the actual path you obtained\n",
    "graphviz_bin_path = '/usr/local/bin'\n",
    "\n",
    "# Add the Graphviz bin directory to the PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + graphviz_bin_path\n",
    "\n",
    "# Now try to plot the tree\n",
    "\n",
    "# KC House Data\n",
    "df = pd.read_csv('datasets/kc_house_data.csv')\n",
    "df_train = df[['bedrooms', 'bathrooms', 'sqft_living', 'floors', 'waterfront', 'view', 'grade', 'lat', 'yr_built', 'sqft_living15']]\n",
    "\n",
    "X = df_train.values\n",
    "y = df.price.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Fitting XGB regressor model and default base learner is Decision Tree\n",
    "xgb_reg = xgb.XGBRegressor(objective=\"reg:linear\", n_estimators=75, subsample=0.75, max_depth=7)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Making Predictions\n",
    "predictions = xgb_reg.predict(X_test)\n",
    "\n",
    "# Variance_score\n",
    "print((explained_variance_score(predictions, y_test)))\n",
    "\n",
    "# To convert data table into a matrix\n",
    "kc_dmatrix = xgb.DMatrix(data=df_train, label=y, feature_names=list(df_train.columns))\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\": \"reg:linear\", \"max_depth\": 2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=kc_dmatrix, num_boost_round=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T01:54:24.109571Z",
     "start_time": "2023-12-08T01:54:23.395747Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "--variance from xgboost regression with decision tree as base learner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2. Linear Base Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 6.446581\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Fetch the Boston housing dataset from the original source\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "# Train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Convert the training and testing sets into DMatrixes\n",
    "boston_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "boston_test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Parameters with booster as gblinear for Linear base learner\n",
    "params = {\"booster\": \"gblinear\", \"objective\": \"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=boston_train, num_boost_round=5)\n",
    "\n",
    "# Making predictions\n",
    "predictions = xg_reg.predict(boston_test)\n",
    "\n",
    "# Computing RMSE\n",
    "print(\"RMSE: %f\" % (np.sqrt(mean_squared_error(y_test, predictions))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T01:54:28.129587Z",
     "start_time": "2023-12-08T01:54:27.083816Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "--rmse using xgboost regression with linear base learner"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "venv",
   "language": "python",
   "display_name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
